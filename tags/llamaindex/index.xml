<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LlamaIndex on If you torture the data long enough, it will confess ©</title>
    <link>/tags/llamaindex/</link>
    <description>Recent content in LlamaIndex on If you torture the data long enough, it will confess ©</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Jan 2024 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/llamaindex/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Applying OpenAI&#39;s RAG Strategies</title>
      <link>/post/2024-01-13-applying-openai-s-rag-strategies/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-applying-openai-s-rag-strategies/</guid>
      <description>&lt;p&gt;At their demo day, Open AI reported a series of RAG experiments for a customer that they worked with. While evaluation metics will depend on your specific application, it’s interesting to see what worked and what didn&amp;rsquo;t for them. Below, we expand on each method mention and show how you can implement each one for yourself. The ability to understand and these methods on your application is critical: from talking to many partners and users, there is no &amp;ldquo;one-size-fits-all&amp;rdquo; solution because different problems require different retrieval techniques.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.langchain.dev/applying-openai-rag/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPT Researcher - Tavily</title>
      <link>/post/2024-01-13-gpt-researcher-tavily/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-gpt-researcher-tavily/</guid>
      <description>&lt;p&gt;The agent can produce detailed, factual and unbiased research reports, with customization options for focusing on relevant resources, outlines, and lessons. Inspired by the recent Plan-and-Solve and RAG papers, GPT Researcher addresses issues of speed, determinism and reliability, offering a more stable performance and increased speed through parallelized agent work, as opposed to synchronous operations.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.tavily.com/docs/gpt-researcher/introduction&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/run-llama/llama-hub/blob/main/llama_hub/tools/notebooks/tavily.ipynb&#34;&gt;Building a Tavily Data Agent&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/langchain-ai/langchain/tree/master/templates/research-assistant&#34;&gt;research-assistan&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Query Pipelines</title>
      <link>/post/2024-01-13-introducing-query-pipelines/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-introducing-query-pipelines/</guid>
      <description>&lt;p&gt;Today we introduce Query Pipelines, a new declarative API within LlamaIndex that allows you to concisely orchestrate simple-to-advanced query workflows over your data for different use cases (RAG, structured data extraction, and more).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.llamaindex.ai/introducing-query-pipelines-025dc2bb0537&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/latest/module_guides/querying/pipeline/usage_pattern.html#&#34;&gt;Usage Pattern&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LlamaHub</title>
      <link>/post/2024-01-13-llamahub/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-llamahub/</guid>
      <description>&lt;p&gt;Get your RAG application rolling in no time.
Mix and match our Data Loaders and Agent Tools to build custom RAG apps or use our LlamaPacks as a starting point for your retrieval use cases.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://llamahub.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LlamaIndex - Ingestion Pipeline</title>
      <link>/post/2024-01-13-llamaindex-ingestion-pipeline/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-llamaindex-ingestion-pipeline/</guid>
      <description>&lt;p&gt;An IngestionPipeline uses a concept of Transformations that are applied to input data. These Transformations are applied to your input data, and the resulting nodes are either returned or inserted into a vector database (if given). Each node+transformation pair is cached, so that subsequent runs (if the cache is persisted) with the same node+transformation combination can use the cached result and save you time.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/root.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://llamahub.ai/l/llama_packs-redis_ingestion_pipeline?from=llama_packs&#34;&gt;LlamaPack&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RAG Pipeline with Query Rewriting</title>
      <link>/post/2024-01-13-rag-pipeline-with-query-rewriting/</link>
      <pubDate>Sat, 13 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/post/2024-01-13-rag-pipeline-with-query-rewriting/</guid>
      <description>&lt;p&gt;Here we chain together a full RAG pipeline consisting of query rewriting, retrieval, reranking, and response synthesis.&lt;/p&gt;
&lt;p&gt;Here we can’t use chain syntax because certain modules depend on multiple inputs (for instance, response synthesis expects both the retrieved nodes and the original question). Instead we’ll construct a DAG explicitly, through add_modules and then add_link.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.llamaindex.ai/en/latest/examples/pipeline/query_pipeline.html#&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss> 
