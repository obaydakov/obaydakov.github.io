<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on If you torture the data long enough, it will confess ¬©</title>
    <link>/categories/deep-learning/</link>
    <description>Recent content in deep-learning on If you torture the data long enough, it will confess ¬©</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Apr 2023 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Transformer, explained in detail</title>
      <link>/post/2023-04-22-transformer-explained-in-detail/</link>
      <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-04-22-transformer-explained-in-detail/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=iOrNbK2T92M&amp;amp;ab_channel=IgorKotenkov&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building LLM applications for production</title>
      <link>/post/2023-04-16-building-llm-applications-for-production/</link>
      <pubDate>Sun, 16 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-04-16-building-llm-applications-for-production/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://huyenchip.com/2023/04/11/llm-engineering.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your end-to-end stack for cloud compute</title>
      <link>/post/2023-04-16-your-end-to-end-stack-for-cloud-compute/</link>
      <pubDate>Sun, 16 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-04-16-your-end-to-end-stack-for-cloud-compute/</guid>
      <description>&lt;p&gt;Modal lets you run or deploy machine learning models, massively parallel compute jobs, task queues, web apps, and much more, without your own infrastructure.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://modal.com/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TRANSFORMER MODELS: AN INTRODUCTION AND CATALOG</title>
      <link>/post/2023-04-08-transformer-models-an-introduction-and-catalog/</link>
      <pubDate>Sat, 08 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-04-08-transformer-models-an-introduction-and-catalog/</guid>
      <description>&lt;p&gt;In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2302.07730.pdf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Comparing Tools For Data Processing Pipelines</title>
      <link>/post/2023-03-19-comparing-tools-for-data-processing-pipelines/</link>
      <pubDate>Sun, 19 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-19-comparing-tools-for-data-processing-pipelines/</guid>
      <description>&lt;p&gt;If you will ask data professionals about what is the most challenging part of their day to day work, you will likely discover their concerns around managing different aspects of data before they get to graduate to the data modeling stage.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neptune.ai/blog/comparing-tools-for-data-processing-pipelines&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchMultimodal (Beta Release</title>
      <link>/post/2023-03-17-torchmultimodal-beta-release/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-17-torchmultimodal-beta-release/</guid>
      <description>&lt;p&gt;TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/multimodal&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Awesome Production Machine Learning</title>
      <link>/post/2023-03-06-awesome-production-machine-learning/</link>
      <pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-06-awesome-production-machine-learning/</guid>
      <description>&lt;p&gt;This repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale and secure your production machine learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/EthicalML/awesome-production-machine-learning/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Annotated Transformer</title>
      <link>/post/2023-03-06-the-annotated-transformer/</link>
      <pubDate>Mon, 06 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-06-the-annotated-transformer/</guid>
      <description>&lt;p&gt;The Transformer has been on a lot of people‚Äôs minds over the last year five years. This post presents an annotated version of the paper in the form of a line-by-line implementation. It reorders and deletes some sections from the original paper and adds comments throughout. This document itself is a working notebook, and should be a completely usable implementation. Code is available here.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nlp.seas.harvard.edu/annotated-transformer/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Custom langchain Agent &amp; Tools with memory</title>
      <link>/post/2023-03-03-custom-langchain-agent-tools-with-memory/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-03-custom-langchain-agent-tools-with-memory/</guid>
      <description>&lt;p&gt;We will build a web app with Streamlit UI which features 4 Python functions as custom Langchain tools. Our agent will also have a short term conversational memory.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NIG8lXk0ULg&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>deepfakes</title>
      <link>/post/2023-03-03-deepfakes/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-03-deepfakes/</guid>
      <description>&lt;p&gt;Here are 105 public repositories matching this topic&amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/topics/deepfakes&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Incredibile pyTorch</title>
      <link>/post/2023-03-03-incredibile-pytorch/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-03-incredibile-pytorch/</guid>
      <description>&lt;p&gt;This is a curated list of tutorials, projects, libraries, videos, papers, books and anything related to the incredible PyTorch. Feel free to make a pull request to contribute to this list.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ritchieng/the-incredible-pytorch&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection</title>
      <link>/post/2023-03-03-yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-spatio-temporal-action-detection/</link>
      <pubDate>Fri, 03 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-03-03-yowov2-a-stronger-yet-efficient-multi-level-detection-framework-for-real-time-spatio-temporal-action-detection/</guid>
      <description>&lt;p&gt;YOWOv2: A Stronger yet Efficient Multi-level Detection Framework for Real-time Spatio-temporal Action Detection&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/yjh0410/YOWOv2&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> OpenAI x Pinecone Q&amp;A app</title>
      <link>/post/2023-02-11-openai-x-pinecone-q-a-app/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-openai-x-pinecone-q-a-app/</guid>
      <description>&lt;p&gt;In this repository we have notebooks and source code used to build the OpenAI x Pinecone Q&amp;amp;A app. You can find more information in our webinar here.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/pinecone-io/examples/tree/master/integrations/openai/beyond_search_webinar&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A command-line utility to train and deploy Machine Learning/Deep Learning models on AWS SageMaker in a few simple steps!</title>
      <link>/post/2023-02-11-a-command-line-utility-to-train-and-deploy-machine-learning-deep-learning-models-on-aws-sagemaker-in-a-few-simple-steps/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-a-command-line-utility-to-train-and-deploy-machine-learning-deep-learning-models-on-aws-sagemaker-in-a-few-simple-steps/</guid>
      <description>&lt;p&gt;We&amp;rsquo;ll provide you with some examples of how Sagify can simplify and expedite your ML pipelines. You can train, tune and deploy a Machine Learning on the same day by using Sagify!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kenza-ai.github.io/sagify/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Design and Deploy Large Language Model Apps</title>
      <link>/post/2023-02-11-design-and-deploy-large-language-model-apps/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-design-and-deploy-large-language-model-apps/</guid>
      <description>&lt;p&gt;Built on years of experience working with large language models.
With one goal, help accelerate their deployment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FUTUREPEDIA</title>
      <link>/post/2023-02-11-futurepedia/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-futurepedia/</guid>
      <description>&lt;p&gt;THE LARGEST AI TOOLS DIRECTORY, UPDATED DAILY&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.futurepedia.io/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenAI Cookbook</title>
      <link>/post/2023-02-11-openai-cookbook/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-openai-cookbook/</guid>
      <description>&lt;p&gt;The OpenAI Cookbook shares example code for accomplishing common tasks with the OpenAI API.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenAI v2 Embedding &#43; Search &#43; Context enhanced completions</title>
      <link>/post/2023-02-11-openai-v2-embedding-search-context-enhanced-completions/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-openai-v2-embedding-search-context-enhanced-completions/</guid>
      <description>&lt;p&gt;OpenAI v2 Embedding + Search + Context enhanced completions&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/OpsConfig/OpenAI_Lab&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The missing frontend for GPT-3</title>
      <link>/post/2023-02-11-the-missing-frontend-for-gpt-3/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-the-missing-frontend-for-gpt-3/</guid>
      <description>&lt;p&gt;Build GPT-3 powered apps, without writing any code&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://retune.so/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using AI to Identify Ingredients and Suggest Recipes</title>
      <link>/post/2023-02-11-using-ai-to-identify-ingredients-and-suggest-recipes/</link>
      <pubDate>Sat, 11 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-11-using-ai-to-identify-ingredients-and-suggest-recipes/</guid>
      <description>&lt;p&gt;Sous-chef.ai allows users to take photos of ingredients in their fridge/pantry and ask for a user‚Äôs food preferences. Using this information, the app provides customized suggested recipes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@brh373/using-ai-to-identify-ingredients-and-suggest-recipes-95482e2aca7d&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A guide to making the best ML pipeline using AutoGluon 2022 new features</title>
      <link>/post/2023-02-04-a-guide-to-making-the-best-ml-pipeline-using-autogluon-2022-new-features/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-a-guide-to-making-the-best-ml-pipeline-using-autogluon-2022-new-features/</guid>
      <description>&lt;p&gt;how to get the best python open source model without overfitting&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/a-guide-to-making-the-best-ml-pipeline-using-autogluon-2022-new-features-3fcb0b7f47d6&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>awesome-nlp</title>
      <link>/post/2023-02-04-awesome-nlp/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-awesome-nlp/</guid>
      <description>&lt;p&gt;A curated list of resources dedicated to Natural Language Processing&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/keon/awesome-nlp#python&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Awesome-Pytorch-list</title>
      <link>/post/2023-02-04-awesome-pytorch-list/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-awesome-pytorch-list/</guid>
      <description>&lt;p&gt;Awesome-Pytorch-list&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/bharathgs/Awesome-pytorch-list&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build a GitHub support bot with GPT3, LangChain, and Python</title>
      <link>/post/2023-02-04-build-a-github-support-bot-with-gpt3-langchain-and-python/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-build-a-github-support-bot-with-gpt3-langchain-and-python/</guid>
      <description>&lt;p&gt;ChatGPT came out a few months ago and blew everyones‚Äô minds with its ability to answer questions sourced from a broad set of knowledge. Around the time that ChatGPT was demonstrating how powerful large language models could be, the Dagster core team was facing a problem.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dagster.io/blog/chatgpt-langchain&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Analysis with ChatGPT and Jupyter Notebooks</title>
      <link>/post/2023-02-04-data-analysis-with-chatgpt-and-jupyter-notebooks/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-data-analysis-with-chatgpt-and-jupyter-notebooks/</guid>
      <description>&lt;p&gt;The conversational way of generating code with ChatGPT works well with the cell structure of Jupyter Notebooks&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/codefile/data-analysis-with-chatgpt-and-jupyter-notebooks-fa2b03753396&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep-Learning-in-Production</title>
      <link>/post/2023-02-04-deep-learning-in-production/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-deep-learning-in-production/</guid>
      <description>&lt;p&gt;In this repository, I will share some useful notes and references about deploying deep learning-based models in production.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ahkarami/Deep-Learning-in-Production&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPT-3 Voice AI Chatbot in Python</title>
      <link>/post/2023-02-04-gpt-3-voice-ai-chatbot-in-python/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-gpt-3-voice-ai-chatbot-in-python/</guid>
      <description>&lt;p&gt;GPT-3 Voice AI Chatbot in Python&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ABIlhTCaWmE&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit#gid=1158069878&#34;&gt;Excel&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Machine Learning Explainability with PyG</title>
      <link>/post/2023-02-04-graph-machine-learning-explainability-with-pyg/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-graph-machine-learning-explainability-with-pyg/</guid>
      <description>&lt;p&gt;Graph Neural Networks (GNNs) have become increasingly popular for processing graph-structured data, such as social networks, molecular graphs, and knowledge graphs. However, the complex nature of graph-based data and the non-linear relationships between nodes in a graph can make it difficult to understand why a GNN makes a particular prediction. With the rise in popularity of Graph Neural Networks, there also came an increased interest in explaining their predictions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@pytorch_geometric/graph-machine-learning-explainability-with-pyg-ff13cffc23c2&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to replicate ChatGPT with Langchain and GPT-3?</title>
      <link>/post/2023-02-04-how-to-replicate-chatgpt-with-langchain-and-gpt-3/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-how-to-replicate-chatgpt-with-langchain-and-gpt-3/</guid>
      <description>&lt;p&gt;It is well-known that ChatGPT is currently capable of impressive feats. It is likely that many individuals have ideas for utilizing this technology in their own projects. However, it should be noted that ChatGPT does not currently have an official API. Using an unofficial API may result in difficulties.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://ahmadrosid.com/blog/langchain-openai-tutorial&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LangChain Hub</title>
      <link>/post/2023-02-04-langchain-hub/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-langchain-hub/</guid>
      <description>&lt;p&gt;Taking inspiration from Hugging Face Hub, LangChainHub is collection of all artifacts useful for working with LangChain primitives such as prompts, chains and agents. The goal of this repository is to be a central resource for sharing and discovering high quality prompts, chains and agents that combine together to form complex LLM applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain-hub&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Longterm Chat External Sources</title>
      <link>/post/2023-02-04-longterm-chat-external-sources/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-longterm-chat-external-sources/</guid>
      <description>&lt;p&gt;GPT-3 chatbot with long-term memory and external sources.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/daveshap/LongtermChatExternalSources&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PlotGenerator</title>
      <link>/post/2023-02-04-plotgenerator/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-plotgenerator/</guid>
      <description>&lt;p&gt;From any synopsis, generate a solid plot.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/daveshap/PlotGenerator&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python &amp; GPT3 Tutorial</title>
      <link>/post/2023-02-04-python-gpt3-tutorial/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-python-gpt3-tutorial/</guid>
      <description>&lt;p&gt;Public Hello World to get used to Python and GPT-3&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/daveshap/PythonGPT3Tutorial&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch</title>
      <link>/post/2023-02-04-pytorch/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-pytorch/</guid>
      <description>&lt;p&gt;PyTorch Lightning &amp;amp; Lightning Hydra Template are neat. Tensor Puzzles is great intro.&lt;/p&gt;
&lt;p&gt;TorchScale is nice library for making Transformers efficiently. BackPACK is interesting too.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wiki.nikiv.dev/machine-learning/libraries/pytorch&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch Template Project</title>
      <link>/post/2023-02-04-pytorch-template-project/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-pytorch-template-project/</guid>
      <description>&lt;p&gt;PyTorch deep learning project made easy.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/victoresque/pytorch-template&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transformer models: an introduction and catalog‚Ää‚Äî‚Ää2023 Edition</title>
      <link>/post/2023-02-04-transformer-models-an-introduction-and-catalog-2023-edition/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-transformer-models-an-introduction-and-catalog-2023-edition/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TRL - Transformer Reinforcement Learning</title>
      <link>/post/2023-02-04-trl-transformer-reinforcement-learning/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-trl-transformer-reinforcement-learning/</guid>
      <description>&lt;p&gt;With trl you can train transformer language models with Proximal Policy Optimization (PPO). The library is built on top of the transformers library by ü§ó Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point most of decoder architectures and encoder-decoder architectures are supported.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lvwerra/trl?utm_source=substack&amp;amp;utm_medium=email&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Makes a Dialog Agent Useful?</title>
      <link>/post/2023-02-04-what-makes-a-dialog-agent-useful/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-what-makes-a-dialog-agent-useful/</guid>
      <description>&lt;p&gt;The techniques behind ChatGPT: RLHF, IFT, CoT, Red teaming, and more&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/dialog-agents&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing a tokenizer with ChatGPT</title>
      <link>/post/2023-02-04-writing-a-tokenizer-with-chatgpt/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-writing-a-tokenizer-with-chatgpt/</guid>
      <description>&lt;p&gt;This morning I decided to test how good ChatGPT is at generating a non-trivial piece of code. I want to write a complete interpreter along the lines of Robert Nystrom‚Äôs excellent book Crafting Interpreters.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://tagide.com/education/writing-a-tokenizer-with-chatgpt/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ùóóùó¶ùó£: Demonstrate‚ÄìSearch‚ÄìPredict</title>
      <link>/post/2023-02-04-demonstrate-search-predict/</link>
      <pubDate>Sat, 04 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-02-04-demonstrate-search-predict/</guid>
      <description>&lt;p&gt;A framework for composing retrieval models and language models into powerful pipelines that tackle knowledge-intensive tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/stanfordnlp/dsp&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Illustrating Reinforcement Learning from Human Feedback (RLHF)</title>
      <link>/post/2023-01-28-illustrating-reinforcement-learning-from-human-feedback-rlhf/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-illustrating-reinforcement-learning-from-human-feedback-rlhf/</guid>
      <description>&lt;p&gt;Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a &amp;ldquo;good&amp;rdquo; text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/rlhf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lvwerra/trl&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AI made easy for developers</title>
      <link>/post/2023-01-28-ai-made-easy-for-developers/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-ai-made-easy-for-developers/</guid>
      <description>&lt;p&gt;Eden AI provides a unique API connected to the best AI engines&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.edenai.co/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/edenai/edenai-apis&#34;&gt;Github&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BUILDER.AI</title>
      <link>/post/2023-01-28-builder-ai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-builder-ai/</guid>
      <description>&lt;p&gt;We make building an app so easy, anyone can do it&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.builder.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ChatGPT Telegram Bot: Fast. No daily limits. Special chat modes</title>
      <link>/post/2023-01-28-chatgpt-telegram-bot-fast-no-daily-limits-special-chat-modes/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-chatgpt-telegram-bot-fast-no-daily-limits-special-chat-modes/</guid>
      <description>&lt;p&gt;We all love chat.openai.com, but&amp;hellip;&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s TERRIBLY laggy, has daily limits, and is only accessible through an archaic web interface.&lt;/p&gt;
&lt;p&gt;This repo is ChatGPT re-created with GPT-3.5 LLM as Telegram Bot. And it works great.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/karfly/chatgpt_telegram_bot&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>copy.ai</title>
      <link>/post/2023-01-28-copy-ai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-copy-ai/</guid>
      <description>&lt;p&gt;Say &amp;lsquo;goodbye&amp;rsquo; to the blank page for good&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.copy.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Copying Tesla&#39;s Data Engine (for food images)</title>
      <link>/post/2023-01-28-copying-tesla-s-data-engine-for-food-images/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-copying-tesla-s-data-engine-for-food-images/</guid>
      <description>&lt;p&gt;A cooking recipe for building Nutrify&amp;rsquo;s data engine.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mrdbourke.com/copying-teslas-data-engine-for-food-images/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning Tuning Playbook</title>
      <link>/post/2023-01-28-deep-learning-tuning-playbook/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-deep-learning-tuning-playbook/</guid>
      <description>&lt;p&gt;This document is for engineers and researchers (both individuals and teams) interested in maximizing the performance of deep learning models. We assume basic knowledge of machine learning and deep learning concepts.
&lt;a href=&#34;https://github.com/google-research/tuning_playbook&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fine Tuning GPT-3: Building a Custom Q&amp;A Bot Using Embeddings</title>
      <link>/post/2023-01-28-fine-tuning-gpt-3-building-a-custom-q-a-bot-using-embeddings/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-fine-tuning-gpt-3-building-a-custom-q-a-bot-using-embeddings/</guid>
      <description>&lt;p&gt;In this guide, we discuss how to fine-tune GPT-3 to create a factual question-and-answer bot based on additional knowledge.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mlq.ai/fine-tuning-gpt-3-question-answer-bot/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Full Stack Deep learning 2022</title>
      <link>/post/2023-01-28-full-stack-deep-learning-2022/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-full-stack-deep-learning-2022/</guid>
      <description>&lt;p&gt;In this video, we cover the purpose and structure of the Full Stack Deep Learning 2022 course and talk about when ML is the right tool to solve your problems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-Iob-FW5jVM&amp;amp;list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://langchain.readthedocs.io/en/latest/&#34;&gt;LangChain&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generative Question-Answering with Long-Term Memory</title>
      <link>/post/2023-01-28-generative-question-answering-with-long-term-memory/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-generative-question-answering-with-long-term-memory/</guid>
      <description>&lt;p&gt;Generative AI sparked several ‚Äúwow‚Äù moments in 2022. From generative art tools like OpenAI‚Äôs DALL-E 2, Midjourney, and Stable Diffusion, to the next generation of Large Language Models like OpenAI‚Äôs GPT-3.5 generation models, BLOOM, and chatbots like LaMDA and ChatGPT.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pinecone.io/learn/openai-gen-qa/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started with GPT-3 vs. Open Source LLMs - LangChain</title>
      <link>/post/2023-01-28-getting-started-with-gpt-3-vs-open-source-llms-langchain/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-getting-started-with-gpt-3-vs-open-source-llms-langchain/</guid>
      <description>&lt;p&gt;LangChain is a popular framework that allows users to quickly build apps and pipelines around Large Language Models. It integrates directly with OpenAI&amp;rsquo;s GPT-3 and GPT-3.5 models and Hugging Face&amp;rsquo;s open-source alternatives like Google&amp;rsquo;s flan-t5 models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nE2skSRWTTs&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPT-3 Fine Tuning as a Service: Build Your Own Custom AI</title>
      <link>/post/2023-01-28-gpt-3-fine-tuning-as-a-service-build-your-own-custom-ai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-gpt-3-fine-tuning-as-a-service-build-your-own-custom-ai/</guid>
      <description>&lt;p&gt;We&amp;rsquo;re excited to announce our new service offering: GPT-3 fine tuning as a service. If you&amp;rsquo;re looking to achieve better results, reduce latency, and save costs on a wide range of natural language processing (NLP) tasks, we&amp;rsquo;re here to help.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mlq.ai/gpt-3-fine-tuning-as-a-service/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to implement Q&amp;A against your documentation with GPT3, embeddings and Datasette</title>
      <link>/post/2023-01-28-how-to-implement-q-a-against-your-documentation-with-gpt3-embeddings-and-datasette/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-how-to-implement-q-a-against-your-documentation-with-gpt3-embeddings-and-datasette/</guid>
      <description>&lt;p&gt;If you‚Äôve spent any time with GPT-3 or ChatGPT, you‚Äôve likely thought about how useful it would be if you could point them at a specific, current collection of text or documentation and have it use that as part of its input for answering questions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://simonwillison.net/2023/Jan/13/semantic-search-answers/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LIGHTNING TRANSFORMERS</title>
      <link>/post/2023-01-28-lightning-transformers/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-lightning-transformers/</guid>
      <description>&lt;p&gt;LIGHTNING TRANSFORMERS documentation&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lightning-transformers.readthedocs.io/en/latest/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pinecone AI examples</title>
      <link>/post/2023-01-28-pinecone-ai-examples/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-pinecone-ai-examples/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/pinecone-io/examples&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.pinecone.io/docs/examples&#34;&gt;Doc examples&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.pinecone.io/docs/quickstart&#34;&gt;Doc quickstart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.pinecone.io/learn/nlp/&#34;&gt;Natural Language Processing (NLP) for Semantic Search&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Question Answering using Embeddings (OpenAI)</title>
      <link>/post/2023-01-28-question-answering-using-embeddings-openai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-question-answering-using-embeddings-openai/</guid>
      <description>&lt;p&gt;Many use cases require GPT-3 to respond to user questions with insightful answers. For example, a customer support chatbot may need to provide answers to common questions. The GPT models have picked up a lot of general knowledge in training, but we often need to ingest and use a large library of more specific information.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples&#34;&gt;Examples&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>quickchat.ai</title>
      <link>/post/2023-01-28-quickchat-ai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-quickchat-ai/</guid>
      <description>&lt;p&gt;Quickchat is a human-like AI assistant that provides accurate and instant answers to customer questions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quickchat.ai/product&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning for tuning language models ( how to train ChatGPT )</title>
      <link>/post/2023-01-28-reinforcement-learning-for-tuning-language-models-how-to-train-chatgpt/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-reinforcement-learning-for-tuning-language-models-how-to-train-chatgpt/</guid>
      <description>&lt;p&gt;The Large Language Model revolution started with the advent of transformers in 2017. Since then there has been an exponential growth in the models trained. Models with 100B+ parameters have been trained. These pre-trained models have changed the way NLP is done. It is much easier to pick a pre-trained model and fine-tune it for a downstream task ( sentiment, question answering, entity recognition etc.. ) than training a model from scratch. Fine-tuning can be done with a much smaller set of examples than training a model from scratch making the whole process of NLP much easier.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Trudo.ai</title>
      <link>/post/2023-01-28-trudo-ai/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-trudo-ai/</guid>
      <description>&lt;p&gt;Fine tuning NLP models (GPT-3/ChatGPT)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://web.trudo.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oNIHOX_j0qU&#34;&gt;Fine-Tuning GPT-3/ChatGPT and Zapier Integration: A Tutorial for No Code OpenAI Developers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VALL-E</title>
      <link>/post/2023-01-28-vall-e/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-vall-e/</guid>
      <description>&lt;p&gt;An unofficial PyTorch implementation of VALL-E, based on the EnCodec tokenizer.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/enhuiz/vall-e&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Whisper</title>
      <link>/post/2023-01-28-whisper/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-whisper/</guid>
      <description>&lt;p&gt;Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Your AI-powered Amazon Listing Optimization Expert</title>
      <link>/post/2023-01-28-your-ai-powered-amazon-listing-optimization-expert/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-28-your-ai-powered-amazon-listing-optimization-expert/</guid>
      <description>&lt;p&gt;CopyMonkey generates and optimizes Amazon listings in seconds. AI helps place all of the important keywords in your Amazon listing to get you ranking organically on the first page.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://copymonkey.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022 Top Papers in AI ‚Äî A Year of Generative Models</title>
      <link>/post/2023-01-21-2022-top-papers-in-ai-a-year-of-generative-models/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-2022-top-papers-in-ai-a-year-of-generative-models/</guid>
      <description>&lt;p&gt;This year, we see significant progress in the field of generative models. Stable Diffusion üé® creates hyperrealistic art. ChatGPT üí¨ answers questions to the meaning of life. Galactica üß¨ learns humanity‚Äôs scientific knowledge but also reveals the limitations of large language models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chuanenlin.medium.com/2022-top-ai-papers-a-year-of-generative-models-a7dcd9109e39#e29a&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build models, ML components and full stack AI apps</title>
      <link>/post/2023-01-21-build-models-ml-components-and-full-stack-ai-apps/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-build-models-ml-components-and-full-stack-ai-apps/</guid>
      <description>&lt;p&gt;Use Lightning, the hyper-minimalistic framework, to build machine learning components that can plug into existing ML workflows. A Lightning component organizes arbitrary code to run on the cloud, manage its own infrastructure, cloud costs, networking, and more. Focus on component logic and not engineering.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lightning.ai/docs/stable/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph ML in 2023: The State of Affairs</title>
      <link>/post/2023-01-21-graph-ml-in-2023-the-state-of-affairs/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-graph-ml-in-2023-the-state-of-affairs/</guid>
      <description>&lt;p&gt;2022 comes to an end and it is about time to sit down and reflect upon the achievements made in Graph ML as well as to hypothesize about possible breakthroughs in 2023.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to Deploy Diffusion Models</title>
      <link>/post/2023-01-21-how-to-deploy-diffusion-models/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-how-to-deploy-diffusion-models/</guid>
      <description>&lt;p&gt;In this tutorial, you‚Äôll learn how to deploy diffusion models at scale and build a text-to-image generator&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lightning.ai/pages/community/tutorial/deploy-diffusion-models/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Lightning-AI/stable-diffusion-deploy&#34;&gt;Muse&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to Graph Machine Learning</title>
      <link>/post/2023-01-21-introduction-to-graph-machine-learning/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-introduction-to-graph-machine-learning/</guid>
      <description>&lt;p&gt;We first study what graphs are, why they are used, and how best to represent them. We then cover briefly how people learn on graphs, from pre-neural methods (exploring graph features at the same time) to what are commonly called Graph Neural Networks. Lastly, we peek into the world of Transformers for graphs.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/intro-graphml&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open-Assistant</title>
      <link>/post/2023-01-21-open-assistant/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-open-assistant/</guid>
      <description>&lt;p&gt;We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/LAION-AI/Open-Assistant&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The CLRS Algorithmic Reasoning Benchmark</title>
      <link>/post/2023-01-21-the-clrs-algorithmic-reasoning-benchmark/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-the-clrs-algorithmic-reasoning-benchmark/</guid>
      <description>&lt;p&gt;Learning representations of algorithms is an emerging area of machine learning, seeking to bridge concepts from neural networks with classical algorithms. The CLRS Algorithmic Reasoning Benchmark (CLRS) consolidates and extends previous work toward evaluation algorithmic reasoning by providing a suite of implementations of classical algorithms. These algorithms have been selected from the third edition of the standard Introduction to Algorithms by Cormen, Leiserson, Rivest and Stein.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepmind/clrs&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>torchview</title>
      <link>/post/2023-01-21-torchview/</link>
      <pubDate>Sat, 21 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/post/2023-01-21-torchview/</guid>
      <description>&lt;p&gt;Torchview provides visualization of pytorch models in the form of visual graphs. Visualization includes tensors, modules, torch.functions and info such as input/output shapes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mert-kurttutan/torchview&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Deep Learning Framework for Multi-target Prediction</title>
      <link>/post/2022-12-26-a-deep-learning-framework-for-multi-target-prediction/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-a-deep-learning-framework-for-multi-target-prediction/</guid>
      <description>&lt;p&gt;This is the official repository of DeepMTP, a deep learning framework that can be used with multi-target prediction (MTP) problems. MTP can be seen as an umbrella term that cover many subareas of machine learning, which include multi-label classification (MLC), multivariate regression (MTR), multi-task learning (MTL), dyadic prediction (DP), and matrix completion (MC). The implementation is mainly written in Python and uses Pytorch for the implementation of the neural network. The goal is for any user to be able to train a model using only a few lines of code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/diliadis/DeepMTP&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepMind&#39;s AlphaTensor Explained</title>
      <link>/post/2022-12-26-deepmind-s-alphatensor-explained/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-deepmind-s-alphatensor-explained/</guid>
      <description>&lt;p&gt;AlphaTensor is a novel AI solution to discover mathematical algorithms with Reinforcement Learning. Learn everything you need to know about AlphaTensor in this comprehensive introduction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.assemblyai.com/blog/deepminds-alphatensor-explained/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Everything is Connected: Deep Learning on Graphs</title>
      <link>/post/2022-12-26-everything-is-connected-deep-learning-on-graphs/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-everything-is-connected-deep-learning-on-graphs/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/@petarvelickovic6033/featured&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GDL Course</title>
      <link>/post/2022-12-26-gdl-course/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-gdl-course/</guid>
      <description>&lt;p&gt;As part of the African Master‚Äôs in Machine Intelligence (AMMI), we have delivered a course on Geometric Deep Learing (GDL100), which closely follows the contents of our GDL proto-book. We make all materials and artefacts from this course publicly available, as companion material for our proto-book, as well as a way to dive deeper into some of the contents for future iterations of the book.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://geometricdeeplearning.com/lectures/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Notebooks using the Hugging Face libraries </title>
      <link>/post/2022-12-26-notebooks-using-the-hugging-face-libraries/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-notebooks-using-the-hugging-face-libraries/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/notebooks/tree/main/examples&#34;&gt;Hugging Face examples&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Pytorch Geometric tutorial</title>
      <link>/post/2022-12-26-pytorch-geometric-tutorial/</link>
      <pubDate>Mon, 26 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-26-pytorch-geometric-tutorial/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://antoniolonga.github.io/Pytorch_geometric_tutorials/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ChatGPT CLI and Python Wrapper</title>
      <link>/post/2022-12-10-chatgpt-cli-and-python-wrapper/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-chatgpt-cli-and-python-wrapper/</guid>
      <description>&lt;p&gt;ChatGPT Wrapper is an open-source tool unofficial API that lets you interact with ChatGPT in Python and as a CLI.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mmabrouk/chatgpt-wrapper&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DeepNash learns to play Stratego from scratch by combining game theory and model-free deep RL</title>
      <link>/post/2022-12-10-deepnash-learns-to-play-stratego-from-scratch-by-combining-game-theory-and-model-free-deep-rl/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-deepnash-learns-to-play-stratego-from-scratch-by-combining-game-theory-and-model-free-deep-rl/</guid>
      <description>&lt;p&gt;Game-playing artificial intelligence (AI) systems have advanced to a new frontier. Stratego, the classic board game that‚Äôs more complex than chess and Go, and craftier than poker, has now been mastered. Published in Science, we present DeepNash, an AI agent that learned the game from scratch to a human expert level by playing against itself.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GEOTORCH A Deep Learning and Scalable Data Processing Framework for Raster and Spatio-Temporal Datasets</title>
      <link>/post/2022-12-10-geotorch-a-deep-learning-and-scalable-data-processing-framework-for-raster-and-spatio-temporal-datasets/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-geotorch-a-deep-learning-and-scalable-data-processing-framework-for-raster-and-spatio-temporal-datasets/</guid>
      <description>&lt;p&gt;GeoTorch is a python library on top of PyTorch and Apache Sedona for deep learning and scalable data processing focusing on raster imagery and spatio-temporal non-imagery datasets. It has various modules for deep learning and data preprocessing under both categories of datasets. The deep learning module offers ready-to-use datasets, models, and&lt;/p&gt;
&lt;p&gt;[Link]{https://kanchanchy.github.io/geotorch/}&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugging Face Diffusion Models Course</title>
      <link>/post/2022-12-10-hugging-face-diffusion-models-course/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-hugging-face-diffusion-models-course/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/diffusion-models-class&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>I Used ChatGPT to Create an Entire AI Application on AWS</title>
      <link>/post/2022-12-10-i-used-chatgpt-to-create-an-entire-ai-application-on-aws/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-i-used-chatgpt-to-create-an-entire-ai-application-on-aws/</guid>
      <description>&lt;p&gt;This new language model could be the pair programmer of your choice going forward&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/i-used-chatgpt-to-create-an-entire-ai-application-on-aws-5b90e34c3d50&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Time Series Forecasting with Transformers</title>
      <link>/post/2022-12-10-probabilistic-time-series-forecasting-with-transformers/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-probabilistic-time-series-forecasting-with-transformers/</guid>
      <description>&lt;p&gt;Time series forecasting is an essential scientific and business problem and as such has also seen a lot of innovation recently with the use of deep learning based models in addition to the classical methods. An important difference between classical methods like ARIMA and novel deep learning methods is the following.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/time-series-transformers&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text-to-Image: Diffusion, Text Conditioning, Guidance, Latent Space</title>
      <link>/post/2022-12-10-text-to-image-diffusion-text-conditioning-guidance-latent-space/</link>
      <pubDate>Sat, 10 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-12-10-text-to-image-diffusion-text-conditioning-guidance-latent-space/</guid>
      <description>&lt;p&gt;Text-to-image has advanced at a breathless pace in 2021 - 2022, starting with DALL¬∑E, then DALL¬∑E 2, Imagen, and now Stable Diffusion. I dug into a couple of papers to learn more about the space and organized my understanding into a few key concepts&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://eugeneyan.com/writing/text-to-image/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</title>
      <link>/post/2022-11-05-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning/</guid>
      <description>&lt;p&gt;Algebra, Topology, Differential Calculus, and
Optimization Theory
For Computer Science and Machine Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cis.upenn.edu/~jean/math-deep.pdf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Captum - Model Interpretability for PyTorch</title>
      <link>/post/2022-11-05-captum-model-interpretability-for-pytorch/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-captum-model-interpretability-for-pytorch/</guid>
      <description>&lt;p&gt;Captum (‚Äúcomprehension‚Äù in Latin) is an open source, extensible library for model interpretability built on PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://captum.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS 230 ‚Äï Deep Learning</title>
      <link>/post/2022-11-05-cs-230-deep-learning/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-cs-230-deep-learning/</guid>
      <description>&lt;p&gt;Set of illustrated Deep Learning cheatsheets covering the content of the CS 230 class&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started in the World of Stable Diffusion</title>
      <link>/post/2022-11-05-getting-started-in-the-world-of-stable-diffusion/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-getting-started-in-the-world-of-stable-diffusion/</guid>
      <description>&lt;p&gt;In simple words, stable diffusion is a deep learning model that can generate an image given a prompt&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion?utm_source=substack&amp;amp;utm_medium=email&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using graph neural networks to recommend related products</title>
      <link>/post/2022-11-05-using-graph-neural-networks-to-recommend-related-products/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-using-graph-neural-networks-to-recommend-related-products/</guid>
      <description>&lt;p&gt;Recommending related products ‚Äî say, a phone case to go along with a new phone ‚Äî is a fundamental capability of e-commerce sites, one that saves customers time and leads to more satisfying shopping experiences.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.science/blog/using-graph-neural-networks-to-recommend-related-products&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building makemore Part 3: Activations &amp; Gradients, BatchNorm</title>
      <link>/post/2022-10-16-building-makemore-part-3-activations-gradients-batchnorm/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-building-makemore-part-3-activations-gradients-batchnorm/</guid>
      <description>&lt;p&gt;We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you&amp;rsquo;d want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P6sfmUTpUmc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS197 Harvard: AI Research Experiences</title>
      <link>/post/2022-10-16-cs197-harvard-ai-research-experiences/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-cs197-harvard-ai-research-experiences/</guid>
      <description>&lt;p&gt;Once we go from training one model to training hundreds of different models with different hyperparameters, we need to start organizing. We‚Äôre going to break down our organization into three pieces: experiment tracking, hyperparameter search, and configuration setup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1kZCrACh8wHFFAinscHpbaHqMBKeErjOgXMVqKSUZIMU/edit#&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How diffusion models work: the math from scratch</title>
      <link>/post/2022-10-16-how-diffusion-models-work-the-math-from-scratch/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-how-diffusion-models-work-the-math-from-scratch/</guid>
      <description>&lt;p&gt;Diffusion models are a new class of state-of-the-art generative models that generate diverse high-resolution images. They have already attracted a lot of attention after OpenAI, Nvidia and Google managed to train large-scale models. Example architectures that are based on diffusion models are GLIDE, DALLE-2, Imagen, and the full open-source stable diffusion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://theaisummer.com/diffusion-models/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spacetimeformer Multivariate Forecasting</title>
      <link>/post/2022-10-16-spacetimeformer-multivariate-forecasting/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-spacetimeformer-multivariate-forecasting/</guid>
      <description>&lt;p&gt;Spacetimeformer is a Transformer that learns temporal patterns like a time series model and spatial patterns like a Graph Neural Network.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/QData/spacetimeformer&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transfer learning for Time Series Forecasting</title>
      <link>/post/2022-10-16-transfer-learning-for-time-series-forecasting/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-transfer-learning-for-time-series-forecasting/</guid>
      <description>&lt;p&gt;Transfer learning refers to the process of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding üöÄ achievements in Machine Learning üß† and has many practical applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Nixtla/transfer-learning-time-series&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DALL¬∑E Now Available Without Waitlist</title>
      <link>/post/2022-10-01-dall-e-now-available-without-waitlist/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-01-dall-e-now-available-without-waitlist/</guid>
      <description>&lt;p&gt;New users can start creating straight away. Lessons learned from deployment and improvements to our safety systems make wider availability possible.
&lt;a href=&#34;https://openai.com/blog/dall-e-now-available-without-waitlist/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>labml.ai Annotated PyTorch Paper Implementations</title>
      <link>/post/2022-10-01-labml-ai-annotated-pytorch-paper-implementations/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-01-labml-ai-annotated-pytorch-paper-implementations/</guid>
      <description>&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, and the website renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>12 Most Popular NLP Projects of 2022 So Far</title>
      <link>/post/2022-09-25-12-most-popular-nlp-projects-of-2022-so-far/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-12-most-popular-nlp-projects-of-2022-so-far/</guid>
      <description>&lt;p&gt;Natural Language Processing remains one of the hottest topics of 2022. By using GitHub stars (albeit certainly not the only measure) as a proxy for popularity, we took a look at what NLP projects are getting the most traction so far this year, just as we recently did with machine learning projects. It‚Äôs a list with some familiar names but there are plenty of surprises also!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://odsc.com/blog/12-most-popular-nlp-projects-of-2022-so-far/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alpha Connect</title>
      <link>/post/2022-09-25-alpha-connect/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-alpha-connect/</guid>
      <description>&lt;p&gt;Recreating DeepMind&amp;rsquo;s AlphaZero - AI Plays Connect 4&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLkYhK7LiOk0OWeGIRsbJz8kZGWxhrTpRx&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best ML Model Registry Tools</title>
      <link>/post/2022-09-25-best-ml-model-registry-tools/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-best-ml-model-registry-tools/</guid>
      <description>&lt;p&gt;A model registry is a central repository that is used to version control Machine Learning (ML) models. It simply tracks the models while they move between training, production, monitoring, and deployment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neptune.ai/blog/ml-model-registry-best-tools&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Practices for ML Engineering</title>
      <link>/post/2022-09-25-best-practices-for-ml-engineering/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-best-practices-for-ml-engineering/</guid>
      <description>&lt;p&gt;This document is intended to help those with a basic knowledge of machine learning get the benefit of Google&amp;rsquo;s best practices in machine learning. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine¬≠-learned model, then you have the necessary background to read this document.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/guides/rules-of-ml&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MinImagen - Build Your Own Imagen Text-to-Image Model</title>
      <link>/post/2022-09-25-minimagen-build-your-own-imagen-text-to-image-model/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-minimagen-build-your-own-imagen-text-to-image-model/</guid>
      <description>&lt;p&gt;Text-to-Image models have made great strides this year, from DALL-E 2 to the more recent Imagen model. In this tutorial learn how to build a minimal Imagen implementation - MinImagen.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build a GNN-based real-time fraud detection solution using Amazon SageMaker, Amazon Neptune, and the Deep Graph Library</title>
      <link>/post/2022-09-03-build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/</guid>
      <description>&lt;p&gt;We focus on four tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processing a tabular transaction dataset into a heterogeneous graph dataset&lt;/li&gt;
&lt;li&gt;Training a GNN model using SageMaker&lt;/li&gt;
&lt;li&gt;Deploying the trained GNN models as a SageMaker endpoint&lt;/li&gt;
&lt;li&gt;Demonstrating real-time inference for incoming transactions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/machine-learning/build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DALL-E: Inside the Artificial Intelligence program that creates images from textual descriptions</title>
      <link>/post/2022-09-03-dall-e-inside-the-artificial-intelligence-program-that-creates-images-from-textual-descriptions/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-dall-e-inside-the-artificial-intelligence-program-that-creates-images-from-textual-descriptions/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/dall-e-image-generator/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Skops</title>
      <link>/post/2022-09-03-introducing-skops/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-introducing-skops/</guid>
      <description>&lt;p&gt;At Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. 
We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/skops&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi node PyTorch Distributed Training Guide For People In A Hurry</title>
      <link>/post/2022-09-03-multi-node-pytorch-distributed-training-guide-for-people-in-a-hurry/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-multi-node-pytorch-distributed-training-guide-for-people-in-a-hurry/</guid>
      <description>&lt;p&gt;PyTorch is designed to be the framework that&amp;rsquo;s both easy to use and delivers performance at scale. Indeed it has become the most popular deep learning framework, by a mile among the research community. However, despite some lengthy official tutorials and a few helpful community blogs, it is not always clear what exactly has to be done to make your PyTorch training to work across multiple nodes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stable Diffusion with Diffusers</title>
      <link>/post/2022-09-03-stable-diffusion-with-diffusers/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-stable-diffusion-with-diffusers/</guid>
      <description>&lt;p&gt;Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. It is trained on 512x512 images from a subset of the LAION-5B database. LAION-5B is the largest, freely accessible multi-modal dataset that currently exists.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/stable_diffusion&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The spelled-out intro to neural networks and backpropagation: building micrograd</title>
      <link>/post/2022-09-03-the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd/</guid>
      <description>&lt;p&gt;This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?app=desktop&amp;amp;v=VMj-3S1tku0&amp;amp;ab_channel=AndrejKarpathy&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications</title>
      <link>/post/2022-09-03-tutorial-on-denoising-diffusion-based-generative-modeling-foundations-and-applications/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-tutorial-on-denoising-diffusion-based-generative-modeling-foundations-and-applications/</guid>
      <description>&lt;p&gt;This video presents our tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications. This tutorial was originally presented at CVPR 2022 in New Orleans and it received a lot of interest from the research community.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?app=desktop&amp;amp;v=cS6JQpEY9cs&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diffusion Models and Score-matching Models</title>
      <link>/post/2022-07-24-diffusion-models-and-score-matching-models/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-diffusion-models-and-score-matching-models/</guid>
      <description>&lt;p&gt;This repository contains a collection of resources and papers on Diffusion Models and Score-matching Models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/heejkoo/Awesome-Diffusion-Models&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPU Puzzles</title>
      <link>/post/2022-07-24-gpu-puzzles/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-gpu-puzzles/</guid>
      <description>&lt;p&gt;GPU architectures are critical to machine learning, and seem to be becoming even more important every day. However you can be an expert in machine learning without ever touching GPU code. It is a bit weird to be work always through abstraction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/srush/GPU-Puzzles?utm_source=tldrnewsletter&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensor Puzzles</title>
      <link>/post/2022-07-24-tensor-puzzles/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-tensor-puzzles/</guid>
      <description>&lt;p&gt;This is a collection of 16 tensor puzzles. Like chess puzzles these are not meant to simulate the complexity of a real program, but to practice in a simplified environment. Each puzzle asks you to reimplement one function in the NumPy standard library without magic.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/srush/Tensor-Puzzles&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top ten cloud GPU platforms for deep learning</title>
      <link>/post/2022-07-24-top-ten-cloud-gpu-platforms-for-deep-learning/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-top-ten-cloud-gpu-platforms-for-deep-learning/</guid>
      <description>&lt;p&gt;In this article, we explore the services of available cloud GPU platforms with a focus on relevant factors such as pricing, infrastructure, design, performance, support, and security. We use this to present the best platforms to consider for your cloud GPU necessities.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/top-ten-cloud-gpu-platforms-for-deep-learning/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022: A Year Full of Amazing AI papers- A Review</title>
      <link>/post/2022-06-26-2022-a-year-full-of-amazing-ai-papers-a-review/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-06-26-2022-a-year-full-of-amazing-ai-papers-a-review/</guid>
      <description>&lt;p&gt;A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/louisfb01/best_AI_papers_2022&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GraphGPS: Navigating Graph Transformers</title>
      <link>/post/2022-06-26-graphgps-navigating-graph-transformers/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-06-26-graphgps-navigating-graph-transformers/</guid>
      <description>&lt;p&gt;Recipes for cooking the best graph transformers
&lt;a href=&#34;https://towardsdatascience.com/graphgps-navigating-graph-transformers-c2cc223a051c&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PYSKL</title>
      <link>/post/2022-05-28-pyskl/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-28-pyskl/</guid>
      <description>&lt;p&gt;PYSKL is a toolbox focusing on action recognition based on SKeLeton data with PYTorch. Various algorithms will be supported for skeleton-based action recognition. We build this project based on the OpenSource Project MMAction2.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kennymckormick/pyskl&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A library to inspect itermediate layers of PyTorch models.</title>
      <link>/post/2022-05-02-a-library-to-inspect-itermediate-layers-of-pytorch-models/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-a-library-to-inspect-itermediate-layers-of-pytorch-models/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s often the case that we want to inspect intermediate layers of a model without modifying the code e.g. visualize attention matrices of language models, get values from an intermediate layer to feed to another layer, or applying a loss function to intermediate layers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/archinetai/surgeon-pytorch&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS224N: Natural Language Processing with Deep Learning | Winter 2021</title>
      <link>/post/2022-05-02-cs224n-natural-language-processing-with-deep-learning-winter-2021/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-cs224n-natural-language-processing-with-deep-learning-winter-2021/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugging Face Transformers Amazon SageMaker Examples</title>
      <link>/post/2022-05-02-hugging-face-transformers-amazon-sagemaker-examples/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-hugging-face-transformers-amazon-sagemaker-examples/</guid>
      <description>&lt;p&gt;Example Jupyter notebooks that demonstrate how to build, train, and deploy Hugging Face Transformers using Amazon SageMaker and the Amazon SageMaker Python SDK.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/notebooks/tree/main/sagemaker&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimize PyTorch Performance for Speed and Memory Efficiency </title>
      <link>/post/2022-05-02-optimize-pytorch-performance-for-speed-and-memory-efficiency/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-optimize-pytorch-performance-for-speed-and-memory-efficiency/</guid>
      <description>&lt;p&gt;The training/inference processes of deep learning models are involved lots of steps. 
The faster each experiment iteration is, the more we can optimize the whole model prediction performance given limited time and resources. 
I collected and organized several PyTorch tricks and tips to maximize the efficiency of memory usage and minimize the run time. To better leverage these tips, we also need to understand how and why they work.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch and Monai for AI Healthcare Imaging - Python Machine Learning Course</title>
      <link>/post/2022-05-02-pytorch-and-monai-for-ai-healthcare-imaging-python-machine-learning-course/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-pytorch-and-monai-for-ai-healthcare-imaging-python-machine-learning-course/</guid>
      <description>&lt;p&gt;Learn how to use PyTorch, Monai, and Python for computer vision using machine learning. One practical use-case for artificial intelligence is healthcare imaging. In this course, you will improve your machine learning skills by creating an algorithm for automatic liver segmentation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=M3ZWfamWrBM&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conversational AI Chatbot with Transformers in Python</title>
      <link>/post/2022-04-09-conversational-ai-chatbot-with-transformers-in-python/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-conversational-ai-chatbot-with-transformers-in-python/</guid>
      <description>&lt;p&gt;Learn how to use Huggingface transformers library to generate conversational responses with the pretrained DialoGPT model in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conversational Chatbot using Transformers and Streamlit</title>
      <link>/post/2022-04-09-conversational-chatbot-using-transformers-and-streamlit/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-conversational-chatbot-using-transformers-and-streamlit/</guid>
      <description>&lt;p&gt;In this article, we are going to build a Conversational Chatbot app using Transformer (microsoft/DialoGPT-medium model), streamlit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chatbotslife.com/conversational-chatbot-using-transformers-and-streamlit-73d621afde9&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Neural Networks: A learning journey since 2008 ‚Äî Graph Attention Networks</title>
      <link>/post/2022-04-09-graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks/</guid>
      <description>&lt;p&gt;Today we‚Äôll dive into the theory and implementation of the Graph Attention Network (GAT). 
In a nutshell: attention rocks, graphs rock, GAT‚Äôs authors rock!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks-f8c39189e7fc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Peltarion cloud platform makes it easier to get started with, build, and deploy AI for whatever you do</title>
      <link>/post/2022-04-09-the-peltarion-cloud-platform-makes-it-easier-to-get-started-with-build-and-deploy-ai-for-whatever-you-do/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-the-peltarion-cloud-platform-makes-it-easier-to-get-started-with-build-and-deploy-ai-for-whatever-you-do/</guid>
      <description>&lt;p&gt;Our deep learning platform enables you to build and deploy AI models&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://peltarion.com/product&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchMetrics</title>
      <link>/post/2022-04-09-torchmetrics/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-torchmetrics/</guid>
      <description>&lt;p&gt;TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It‚Äôs designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sebastianraschka.com/blog/2022/torchmetrics.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two minutes NLP ‚Äî Quick Introduction to Haystack</title>
      <link>/post/2022-04-09-two-minutes-nlp-quick-introduction-to-haystack/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-two-minutes-nlp-quick-introduction-to-haystack/</guid>
      <description>&lt;p&gt;Question Answering, Semantic Search, and the Retriever-Reader pipeline&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/nlplanet/two-minutes-nlp-quick-introduction-to-haystack-da86d0402998&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack/issues/486&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deepchecks Suite </title>
      <link>/post/2022-03-21-deepchecks-suite/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-deepchecks-suite/</guid>
      <description>&lt;p&gt;Deepchecks is the leading tool for validating your machine learning models and data, and it enables doing so with minimal effort. Deepchecks accompanies you through various validation needs such as verifying your data‚Äôs integrity, inspecting its distributions, validating data splits, evaluating your model and comparing between different models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.deepchecks.com/en/stable/examples/guides/quickstart_in_5_minutes.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing</title>
      <link>/post/2022-03-21-graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing/</guid>
      <description>&lt;p&gt;Physics-inspired continuous learning models on graphs allow to overcome the limitations of traditional GNNs&lt;/p&gt;
&lt;p&gt;The message-passing paradigm has been the ‚Äúbattle horse‚Äù of deep learning on graphs for several years, making graph neural networks a big success in a‚Ä¶&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph-based Fraud Detection Papers and Resources</title>
      <link>/post/2022-03-21-graph-based-fraud-detection-papers-and-resources/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-graph-based-fraud-detection-papers-and-resources/</guid>
      <description>&lt;p&gt;A curated list of fraud detection papers using graph information or graph neural networks&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/safe-graph/graph-fraud-detection-papers&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch VAE</title>
      <link>/post/2022-03-21-pytorch-vae/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-pytorch-vae/</guid>
      <description>&lt;p&gt;A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility.&lt;/p&gt;
&lt;p&gt;The aim of this project is to provide a quick and simple working example for many of the cool VAE models out there. All the models are trained on the CelebA dataset for consistency and comparison.&lt;/p&gt;
&lt;p&gt;The architecture of all the models are kept as similar as possible with the same layers, except for cases where the original paper necessitates a radically different architecture (Ex. VQ VAE uses Residual layers and no Batch-Norm, unlike other models). Here are the results of each model.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AntixK/PyTorch-VAE&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simplifying Transformer Research with xFormers &amp; Lightning</title>
      <link>/post/2022-03-21-simplifying-transformer-research-with-xformers-lightning/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-simplifying-transformer-research-with-xformers-lightning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://devblog.pytorchlightning.ai/part-i-simplifying-transformer-research-with-xformers-lightning-a715737b8ad4&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VAE Playground</title>
      <link>/post/2022-03-21-vae-playground/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-vae-playground/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/reoneo97/vae-playground&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Guide to Transformer Neural Networks</title>
      <link>/post/2022-03-21-visual-guide-to-transformer-neural-networks/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-visual-guide-to-transformer-neural-networks/</guid>
      <description>&lt;p&gt;Visual Guide to Transformer Neural Networks (Series) - Step by Step Intuitive Explanation&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gJ9kaJsE78k&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yann LeCun: &#34;A Path Towards Autonomous AI&#34;</title>
      <link>/post/2022-03-21-yann-lecun-a-path-towards-autonomous-ai/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-yann-lecun-a-path-towards-autonomous-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DokLw1tILlw&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Flask Web App for Automatic Text Summarization Using SBERT</title>
      <link>/post/2022-02-27-a-flask-web-app-for-automatic-text-summarization-using-sbert/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-27-a-flask-web-app-for-automatic-text-summarization-using-sbert/</guid>
      <description>&lt;p&gt;In this blog, we will build a Flask web app that can input any long piece of information such as a blog or news article and summarize it into just five lines!&lt;/p&gt;
&lt;p&gt;Text summarization is an NLP(Natural Language Processing) task. SBERT(Sentence-BERT) has been used to achieve the same.&lt;/p&gt;
&lt;p&gt;By the end of the article, you will learn how to integrate AI models and specifically pre-trained BERT models with Flask web technology as well! I will be explaining the step-by-step implementation right from the setup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2022/02/a-flask-web-app-for-automatic-text-summarization-using-sbert/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What does 2022 hold for Geometric &amp; Graph ML?</title>
      <link>/post/2022-02-27-what-does-2022-hold-for-geometric-graph-ml/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-27-what-does-2022-hold-for-geometric-graph-ml/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time face swap fpr PC streamig or video calls</title>
      <link>/post/2022-02-19-real-time-face-swap-fpr-pc-streamig-or-video-calls/</link>
      <pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-19-real-time-face-swap-fpr-pc-streamig-or-video-calls/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/iperov/DeepFaceLive&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The incredible pytorch</title>
      <link>/post/2022-02-19-the-incredible-pytorch/</link>
      <pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-19-the-incredible-pytorch/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.ritchieng.com/the-incredible-pytorch/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> AlphaFold2 Talk</title>
      <link>/post/2021-12-30-alphafold2-talk/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-alphafold2-talk/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/BurkovBA/AlphaFold2-talk&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A powerful and flexible machine learning platform for drug discovery</title>
      <link>/post/2021-12-30-a-powerful-and-flexible-machine-learning-platform-for-drug-discovery/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-a-powerful-and-flexible-machine-learning-platform-for-drug-discovery/</guid>
      <description>&lt;p&gt;TorchDrug is a machine learning platform designed for drug discovery, covering techniques from graph machine learning (graph neural networks, geometric deep learning &amp;amp; knowledge graphs), deep generative models to reinforcement learning. It provides a comprehensive and flexible interface to support rapid prototyping of drug discovery models in PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://torchdrug.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geometric Deep Learing</title>
      <link>/post/2021-12-30-geometric-deep-learing/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-geometric-deep-learing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://geometricdeeplearning.com/lectures/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph ML in 2022: Where Are We Now?</title>
      <link>/post/2021-12-30-graph-ml-in-2022-where-are-we-now/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-graph-ml-in-2022-where-are-we-now/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KG Course 2021</title>
      <link>/post/2021-12-30-kg-course-2021/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-kg-course-2021/</guid>
      <description>&lt;p&gt;–ö—É—Ä—Å –ø–æ –≥—Ä–∞—Ñ–∞–º –∑–Ω–∞–Ω–∏–π (Knowledge Graphs) –∏ –∫–∞–∫ –∏—Ö –≥–æ—Ç–æ–≤–∏—Ç—å –≤ 2021 –≥–æ–¥—É.
–ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://migalkin.github.io/kgcourse2021/#syllabus&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.</title>
      <link>/post/2021-12-30-pyg-pytorch-geometric-is-a-library-built-upon-pytorch-to-easily-write-and-train-graph-neural-networks-gnns-for-a-wide-range-of-applications-related-to-structured-data/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-pyg-pytorch-geometric-is-a-library-built-upon-pytorch-to-easily-write-and-train-graph-neural-networks-gnns-for-a-wide-range-of-applications-related-to-structured-data/</guid>
      <description>&lt;p&gt;It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers. In addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support, distributed graph learning via Quiver, a large number of common benchmark datasets (based on simple interfaces to create your own), the GraphGym experiment manager, and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pytorch-geometric.readthedocs.io/en/latest/index.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyKEEN is a Python package for reproducible, facile knowledge graph embeddings.</title>
      <link>/post/2021-12-30-pykeen-is-a-python-package-for-reproducible-facile-knowledge-graph-embeddings/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-pykeen-is-a-python-package-for-reproducible-facile-knowledge-graph-embeddings/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pykeen.readthedocs.io/en/stable/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Complete Intuitive Guide To Transfer Learning </title>
      <link>/post/2021-11-14-a-complete-intuitive-guide-to-transfer-learning/</link>
      <pubDate>Sun, 14 Nov 2021 10:42:38 +0200</pubDate>
      
      <guid>/post/2021-11-14-a-complete-intuitive-guide-to-transfer-learning/</guid>
      <description>&lt;p&gt;Advancements in deep learning have been rapid over the past decade.&lt;/p&gt;
&lt;p&gt;While the discovery of neural networks happened almost six decades ago with the invention of the first artificial neural network in 1958 by psychologist Frank Rosenblatt (called the &amp;ldquo;perceptron&amp;rdquo;), the developments in the field did not gain true popularity until about a decade ago.&lt;/p&gt;
&lt;p&gt;The most popular achievement in 2009 was the creation of ImageNet. ImageNet is a humungous visual dataset that has led to some of the best modern-day deep learning and computer vision projects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/transfer-learning-explained/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to Anomaly Detection with Autoencoders</title>
      <link>/post/2021-11-14-a-gentle-introduction-to-anomaly-detection-with-autoencoders/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-a-gentle-introduction-to-anomaly-detection-with-autoencoders/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/autoencoder.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Anomagram is an interactive visualization tool for exploring how a deep learning model can be applied to the task of anomaly detection (on stationary data).&lt;/p&gt;
&lt;p&gt;Given an ECG signal sample, an autoencoder model (running live in your browser) can predict if it is normal or abnormal.&lt;/p&gt;
&lt;p&gt;To try it out, click any of the test ECG signals from the ECG5000 dataset below, or better still, draw a signal to see the model&amp;rsquo;s prediction!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://anomagram.fastforwardlabs.com/#/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Visual Guide to Using BERT for the First Time</title>
      <link>/post/2021-11-14-a-visual-guide-to-using-bert-for-the-first-time/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-a-visual-guide-to-using-bert-for-the-first-time/</guid>
      <description>&lt;p&gt;This post is a simple tutorial for how to use a variant of BERT to classify sentences.&lt;/p&gt;
&lt;p&gt;This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BMW TechOffice MUNICH</title>
      <link>/post/2021-11-14-bmw-techoffice-munich/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-bmw-techoffice-munich/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/repo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This organization contains software for realtime computer vision published by the members, partners and friends of the BMW TechOffice MUNICH and InnovationLab.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BMW-InnovationLab&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Face Verification With Keras and Streamlit</title>
      <link>/post/2021-11-14-face-verification-with-keras-and-streamlit/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-face-verification-with-keras-and-streamlit/</guid>
      <description>&lt;p&gt;Streamlit enables data scientists and machine learning practitioners to build data and machine learning applications quickly.&lt;/p&gt;
&lt;p&gt;In this piece, we will look at how we can use Streamlit to build a face verification application.&lt;/p&gt;
&lt;p&gt;However, before we can start verifying faces, we have to detect them. In computer vision, face detection is the task of locating and localizing faces in an image. Face verification is the process of comparing the similarity of two or more images.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/face-verification-with-keras/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to fine-tune BERT to classify your Slack chats without coding</title>
      <link>/post/2021-11-14-how-to-fine-tune-bert-to-classify-your-slack-chats-without-coding/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-how-to-fine-tune-bert-to-classify-your-slack-chats-without-coding/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/lifecycle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Slack chats can become messy with time, proving difficult to extract meaningful information.&lt;/p&gt;
&lt;p&gt;In this article, I want to present a quick codeless way of fine-tuning and deploying the commonly used BERT classifier to do conversational analysis.&lt;/p&gt;
&lt;p&gt;We will use that system to extract tasks, facts, and other valuable information from our Slack conversations.&lt;/p&gt;
&lt;p&gt;It could be easily extended for categorizing any other textual data, like support requests, emails, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/how-to-finetune-bert-to-classify-your-slack-chats-without-coding-3a7002936bcf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streamlit Demo: The Udacity Self-driving Car Image Browser</title>
      <link>/post/2021-11-14-streamlit-demo-the-udacity-self-driving-car-image-browser/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-streamlit-demo-the-udacity-self-driving-car-image-browser/</guid>
      <description>&lt;p&gt;This project demonstrates the Udacity self-driving-car dataset and YOLO object detection into an interactive Streamlit app.&lt;/p&gt;
&lt;p&gt;The complete demo is implemented in less than 300 lines of Python and illustrates all the major building blocks of Streamlit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamlit/demo-self-driving&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vision Transformers Explained</title>
      <link>/post/2021-11-14-vision-transformers-explained/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-vision-transformers-explained/</guid>
      <description>&lt;p&gt;Introduced in the paper, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Vision Transformers (ViT) are the new talk of the town for SOTA image classification.&lt;/p&gt;
&lt;p&gt;Experts feel this is only the tip of the iceberg when it comes to Transformer architectures replacing their convolutional counterparts for upstream/downstream tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/vision-transformers/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss> 
