<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep-learning on If you torture the data long enough, it will confess ¬©</title>
    <link>/categories/deep-learning/</link>
    <description>Recent content in deep-learning on If you torture the data long enough, it will confess ¬©</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 05 Nov 2022 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</title>
      <link>/post/2022-11-05-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-algebra-topology-differential-calculus-and-optimization-theory-for-computer-science-and-machine-learning/</guid>
      <description>&lt;p&gt;Algebra, Topology, Differential Calculus, and
Optimization Theory
For Computer Science and Machine Learning&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.cis.upenn.edu/~jean/math-deep.pdf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Captum - Model Interpretability for PyTorch</title>
      <link>/post/2022-11-05-captum-model-interpretability-for-pytorch/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-captum-model-interpretability-for-pytorch/</guid>
      <description>&lt;p&gt;Captum (‚Äúcomprehension‚Äù in Latin) is an open source, extensible library for model interpretability built on PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://captum.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS 230 ‚Äï Deep Learning</title>
      <link>/post/2022-11-05-cs-230-deep-learning/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-cs-230-deep-learning/</guid>
      <description>&lt;p&gt;Set of illustrated Deep Learning cheatsheets covering the content of the CS 230 class&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://stanford.edu/~shervine/teaching/cs-230/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting Started in the World of Stable Diffusion</title>
      <link>/post/2022-11-05-getting-started-in-the-world-of-stable-diffusion/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-getting-started-in-the-world-of-stable-diffusion/</guid>
      <description>&lt;p&gt;In simple words, stable diffusion is a deep learning model that can generate an image given a prompt&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://bipinkrishnan.github.io/posts/getting-started-in-the-world-of-stable-diffusion?utm_source=substack&amp;amp;utm_medium=email&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using graph neural networks to recommend related products</title>
      <link>/post/2022-11-05-using-graph-neural-networks-to-recommend-related-products/</link>
      <pubDate>Sat, 05 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-11-05-using-graph-neural-networks-to-recommend-related-products/</guid>
      <description>&lt;p&gt;Recommending related products ‚Äî say, a phone case to go along with a new phone ‚Äî is a fundamental capability of e-commerce sites, one that saves customers time and leads to more satisfying shopping experiences.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.science/blog/using-graph-neural-networks-to-recommend-related-products&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building makemore Part 3: Activations &amp; Gradients, BatchNorm</title>
      <link>/post/2022-10-16-building-makemore-part-3-activations-gradients-batchnorm/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-building-makemore-part-3-activations-gradients-batchnorm/</guid>
      <description>&lt;p&gt;We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you&amp;rsquo;d want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P6sfmUTpUmc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS197 Harvard: AI Research Experiences</title>
      <link>/post/2022-10-16-cs197-harvard-ai-research-experiences/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-cs197-harvard-ai-research-experiences/</guid>
      <description>&lt;p&gt;Once we go from training one model to training hundreds of different models with different hyperparameters, we need to start organizing. We‚Äôre going to break down our organization into three pieces: experiment tracking, hyperparameter search, and configuration setup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1kZCrACh8wHFFAinscHpbaHqMBKeErjOgXMVqKSUZIMU/edit#&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How diffusion models work: the math from scratch</title>
      <link>/post/2022-10-16-how-diffusion-models-work-the-math-from-scratch/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-how-diffusion-models-work-the-math-from-scratch/</guid>
      <description>&lt;p&gt;Diffusion models are a new class of state-of-the-art generative models that generate diverse high-resolution images. They have already attracted a lot of attention after OpenAI, Nvidia and Google managed to train large-scale models. Example architectures that are based on diffusion models are GLIDE, DALLE-2, Imagen, and the full open-source stable diffusion.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://theaisummer.com/diffusion-models/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spacetimeformer Multivariate Forecasting</title>
      <link>/post/2022-10-16-spacetimeformer-multivariate-forecasting/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-spacetimeformer-multivariate-forecasting/</guid>
      <description>&lt;p&gt;Spacetimeformer is a Transformer that learns temporal patterns like a time series model and spatial patterns like a Graph Neural Network.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/QData/spacetimeformer&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transfer learning for Time Series Forecasting</title>
      <link>/post/2022-10-16-transfer-learning-for-time-series-forecasting/</link>
      <pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-16-transfer-learning-for-time-series-forecasting/</guid>
      <description>&lt;p&gt;Transfer learning refers to the process of pre-training a flexible model on a large dataset and using it later on other data with little to no training. It is one of the most outstanding üöÄ achievements in Machine Learning üß† and has many practical applications.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Nixtla/transfer-learning-time-series&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DALL¬∑E Now Available Without Waitlist</title>
      <link>/post/2022-10-01-dall-e-now-available-without-waitlist/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-01-dall-e-now-available-without-waitlist/</guid>
      <description>&lt;p&gt;New users can start creating straight away. Lessons learned from deployment and improvements to our safety systems make wider availability possible.
&lt;a href=&#34;https://openai.com/blog/dall-e-now-available-without-waitlist/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>labml.ai Annotated PyTorch Paper Implementations</title>
      <link>/post/2022-10-01-labml-ai-annotated-pytorch-paper-implementations/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-10-01-labml-ai-annotated-pytorch-paper-implementations/</guid>
      <description>&lt;p&gt;This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, and the website renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://nn.labml.ai/index.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>12 Most Popular NLP Projects of 2022 So Far</title>
      <link>/post/2022-09-25-12-most-popular-nlp-projects-of-2022-so-far/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-12-most-popular-nlp-projects-of-2022-so-far/</guid>
      <description>&lt;p&gt;Natural Language Processing remains one of the hottest topics of 2022. By using GitHub stars (albeit certainly not the only measure) as a proxy for popularity, we took a look at what NLP projects are getting the most traction so far this year, just as we recently did with machine learning projects. It‚Äôs a list with some familiar names but there are plenty of surprises also!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://odsc.com/blog/12-most-popular-nlp-projects-of-2022-so-far/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alpha Connect</title>
      <link>/post/2022-09-25-alpha-connect/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-alpha-connect/</guid>
      <description>&lt;p&gt;Recreating DeepMind&amp;rsquo;s AlphaZero - AI Plays Connect 4&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLkYhK7LiOk0OWeGIRsbJz8kZGWxhrTpRx&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best ML Model Registry Tools</title>
      <link>/post/2022-09-25-best-ml-model-registry-tools/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-best-ml-model-registry-tools/</guid>
      <description>&lt;p&gt;A model registry is a central repository that is used to version control Machine Learning (ML) models. It simply tracks the models while they move between training, production, monitoring, and deployment.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://neptune.ai/blog/ml-model-registry-best-tools&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Best Practices for ML Engineering</title>
      <link>/post/2022-09-25-best-practices-for-ml-engineering/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-best-practices-for-ml-engineering/</guid>
      <description>&lt;p&gt;This document is intended to help those with a basic knowledge of machine learning get the benefit of Google&amp;rsquo;s best practices in machine learning. It presents a style for machine learning, similar to the Google C++ Style Guide and other popular guides to practical programming. If you have taken a class in machine learning, or built or worked on a machine¬≠-learned model, then you have the necessary background to read this document.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/machine-learning/guides/rules-of-ml&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MinImagen - Build Your Own Imagen Text-to-Image Model</title>
      <link>/post/2022-09-25-minimagen-build-your-own-imagen-text-to-image-model/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-25-minimagen-build-your-own-imagen-text-to-image-model/</guid>
      <description>&lt;p&gt;Text-to-Image models have made great strides this year, from DALL-E 2 to the more recent Imagen model. In this tutorial learn how to build a minimal Imagen implementation - MinImagen.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.assemblyai.com/blog/minimagen-build-your-own-imagen-text-to-image-model/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Build a GNN-based real-time fraud detection solution using Amazon SageMaker, Amazon Neptune, and the Deep Graph Library</title>
      <link>/post/2022-09-03-build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/</guid>
      <description>&lt;p&gt;We focus on four tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Processing a tabular transaction dataset into a heterogeneous graph dataset&lt;/li&gt;
&lt;li&gt;Training a GNN model using SageMaker&lt;/li&gt;
&lt;li&gt;Deploying the trained GNN models as a SageMaker endpoint&lt;/li&gt;
&lt;li&gt;Demonstrating real-time inference for incoming transactions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/machine-learning/build-a-gnn-based-real-time-fraud-detection-solution-using-amazon-sagemaker-amazon-neptune-and-the-deep-graph-library/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DALL-E: Inside the Artificial Intelligence program that creates images from textual descriptions</title>
      <link>/post/2022-09-03-dall-e-inside-the-artificial-intelligence-program-that-creates-images-from-textual-descriptions/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-dall-e-inside-the-artificial-intelligence-program-that-creates-images-from-textual-descriptions/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/dall-e-image-generator/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing Skops</title>
      <link>/post/2022-09-03-introducing-skops/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-introducing-skops/</guid>
      <description>&lt;p&gt;At Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. 
We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/skops&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi node PyTorch Distributed Training Guide For People In A Hurry</title>
      <link>/post/2022-09-03-multi-node-pytorch-distributed-training-guide-for-people-in-a-hurry/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-multi-node-pytorch-distributed-training-guide-for-people-in-a-hurry/</guid>
      <description>&lt;p&gt;PyTorch is designed to be the framework that&amp;rsquo;s both easy to use and delivers performance at scale. Indeed it has become the most popular deep learning framework, by a mile among the research community. However, despite some lengthy official tutorials and a few helpful community blogs, it is not always clear what exactly has to be done to make your PyTorch training to work across multiple nodes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lambdalabs.com/blog/multi-node-pytorch-distributed-training-guide/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stable Diffusion with Diffusers</title>
      <link>/post/2022-09-03-stable-diffusion-with-diffusers/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-stable-diffusion-with-diffusers/</guid>
      <description>&lt;p&gt;Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. It is trained on 512x512 images from a subset of the LAION-5B database. LAION-5B is the largest, freely accessible multi-modal dataset that currently exists.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://huggingface.co/blog/stable_diffusion&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The spelled-out intro to neural networks and backpropagation: building micrograd</title>
      <link>/post/2022-09-03-the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-the-spelled-out-intro-to-neural-networks-and-backpropagation-building-micrograd/</guid>
      <description>&lt;p&gt;This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?app=desktop&amp;amp;v=VMj-3S1tku0&amp;amp;ab_channel=AndrejKarpathy&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications</title>
      <link>/post/2022-09-03-tutorial-on-denoising-diffusion-based-generative-modeling-foundations-and-applications/</link>
      <pubDate>Sat, 03 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-09-03-tutorial-on-denoising-diffusion-based-generative-modeling-foundations-and-applications/</guid>
      <description>&lt;p&gt;This video presents our tutorial on Denoising Diffusion-based Generative Modeling: Foundations and Applications. This tutorial was originally presented at CVPR 2022 in New Orleans and it received a lot of interest from the research community.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?app=desktop&amp;amp;v=cS6JQpEY9cs&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diffusion Models and Score-matching Models</title>
      <link>/post/2022-07-24-diffusion-models-and-score-matching-models/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-diffusion-models-and-score-matching-models/</guid>
      <description>&lt;p&gt;This repository contains a collection of resources and papers on Diffusion Models and Score-matching Models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/heejkoo/Awesome-Diffusion-Models&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GPU Puzzles</title>
      <link>/post/2022-07-24-gpu-puzzles/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-gpu-puzzles/</guid>
      <description>&lt;p&gt;GPU architectures are critical to machine learning, and seem to be becoming even more important every day. However you can be an expert in machine learning without ever touching GPU code. It is a bit weird to be work always through abstraction.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/srush/GPU-Puzzles?utm_source=tldrnewsletter&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tensor Puzzles</title>
      <link>/post/2022-07-24-tensor-puzzles/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-tensor-puzzles/</guid>
      <description>&lt;p&gt;This is a collection of 16 tensor puzzles. Like chess puzzles these are not meant to simulate the complexity of a real program, but to practice in a simplified environment. Each puzzle asks you to reimplement one function in the NumPy standard library without magic.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/srush/Tensor-Puzzles&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Top ten cloud GPU platforms for deep learning</title>
      <link>/post/2022-07-24-top-ten-cloud-gpu-platforms-for-deep-learning/</link>
      <pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-07-24-top-ten-cloud-gpu-platforms-for-deep-learning/</guid>
      <description>&lt;p&gt;In this article, we explore the services of available cloud GPU platforms with a focus on relevant factors such as pricing, infrastructure, design, performance, support, and security. We use this to present the best platforms to consider for your cloud GPU necessities.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/top-ten-cloud-gpu-platforms-for-deep-learning/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022: A Year Full of Amazing AI papers- A Review</title>
      <link>/post/2022-06-26-2022-a-year-full-of-amazing-ai-papers-a-review/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-06-26-2022-a-year-full-of-amazing-ai-papers-a-review/</guid>
      <description>&lt;p&gt;A curated list of the latest breakthroughs in AI by release date with a clear video explanation, link to a more in-depth article, and code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/louisfb01/best_AI_papers_2022&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GraphGPS: Navigating Graph Transformers</title>
      <link>/post/2022-06-26-graphgps-navigating-graph-transformers/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-06-26-graphgps-navigating-graph-transformers/</guid>
      <description>&lt;p&gt;Recipes for cooking the best graph transformers
&lt;a href=&#34;https://towardsdatascience.com/graphgps-navigating-graph-transformers-c2cc223a051c&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PYSKL</title>
      <link>/post/2022-05-28-pyskl/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-28-pyskl/</guid>
      <description>&lt;p&gt;PYSKL is a toolbox focusing on action recognition based on SKeLeton data with PYTorch. Various algorithms will be supported for skeleton-based action recognition. We build this project based on the OpenSource Project MMAction2.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kennymckormick/pyskl&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A library to inspect itermediate layers of PyTorch models.</title>
      <link>/post/2022-05-02-a-library-to-inspect-itermediate-layers-of-pytorch-models/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-a-library-to-inspect-itermediate-layers-of-pytorch-models/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s often the case that we want to inspect intermediate layers of a model without modifying the code e.g. visualize attention matrices of language models, get values from an intermediate layer to feed to another layer, or applying a loss function to intermediate layers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/archinetai/surgeon-pytorch&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CS224N: Natural Language Processing with Deep Learning | Winter 2021</title>
      <link>/post/2022-05-02-cs224n-natural-language-processing-with-deep-learning-winter-2021/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-cs224n-natural-language-processing-with-deep-learning-winter-2021/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hugging Face Transformers Amazon SageMaker Examples</title>
      <link>/post/2022-05-02-hugging-face-transformers-amazon-sagemaker-examples/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-hugging-face-transformers-amazon-sagemaker-examples/</guid>
      <description>&lt;p&gt;Example Jupyter notebooks that demonstrate how to build, train, and deploy Hugging Face Transformers using Amazon SageMaker and the Amazon SageMaker Python SDK.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/notebooks/tree/main/sagemaker&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimize PyTorch Performance for Speed and Memory Efficiency </title>
      <link>/post/2022-05-02-optimize-pytorch-performance-for-speed-and-memory-efficiency/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-optimize-pytorch-performance-for-speed-and-memory-efficiency/</guid>
      <description>&lt;p&gt;The training/inference processes of deep learning models are involved lots of steps. 
The faster each experiment iteration is, the more we can optimize the whole model prediction performance given limited time and resources. 
I collected and organized several PyTorch tricks and tips to maximize the efficiency of memory usage and minimize the run time. To better leverage these tips, we also need to understand how and why they work.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/optimize-pytorch-performance-for-speed-and-memory-efficiency-2022-84f453916ea6&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch and Monai for AI Healthcare Imaging - Python Machine Learning Course</title>
      <link>/post/2022-05-02-pytorch-and-monai-for-ai-healthcare-imaging-python-machine-learning-course/</link>
      <pubDate>Mon, 02 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-05-02-pytorch-and-monai-for-ai-healthcare-imaging-python-machine-learning-course/</guid>
      <description>&lt;p&gt;Learn how to use PyTorch, Monai, and Python for computer vision using machine learning. One practical use-case for artificial intelligence is healthcare imaging. In this course, you will improve your machine learning skills by creating an algorithm for automatic liver segmentation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=M3ZWfamWrBM&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conversational AI Chatbot with Transformers in Python</title>
      <link>/post/2022-04-09-conversational-ai-chatbot-with-transformers-in-python/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-conversational-ai-chatbot-with-transformers-in-python/</guid>
      <description>&lt;p&gt;Learn how to use Huggingface transformers library to generate conversational responses with the pretrained DialoGPT model in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.thepythoncode.com/article/conversational-ai-chatbot-with-huggingface-transformers-in-python&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conversational Chatbot using Transformers and Streamlit</title>
      <link>/post/2022-04-09-conversational-chatbot-using-transformers-and-streamlit/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-conversational-chatbot-using-transformers-and-streamlit/</guid>
      <description>&lt;p&gt;In this article, we are going to build a Conversational Chatbot app using Transformer (microsoft/DialoGPT-medium model), streamlit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://chatbotslife.com/conversational-chatbot-using-transformers-and-streamlit-73d621afde9&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Neural Networks: A learning journey since 2008 ‚Äî Graph Attention Networks</title>
      <link>/post/2022-04-09-graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks/</guid>
      <description>&lt;p&gt;Today we‚Äôll dive into the theory and implementation of the Graph Attention Network (GAT). 
In a nutshell: attention rocks, graphs rock, GAT‚Äôs authors rock!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-graph-attention-networks-f8c39189e7fc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Peltarion cloud platform makes it easier to get started with, build, and deploy AI for whatever you do</title>
      <link>/post/2022-04-09-the-peltarion-cloud-platform-makes-it-easier-to-get-started-with-build-and-deploy-ai-for-whatever-you-do/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-the-peltarion-cloud-platform-makes-it-easier-to-get-started-with-build-and-deploy-ai-for-whatever-you-do/</guid>
      <description>&lt;p&gt;Our deep learning platform enables you to build and deploy AI models&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://peltarion.com/product&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchMetrics</title>
      <link>/post/2022-04-09-torchmetrics/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-torchmetrics/</guid>
      <description>&lt;p&gt;TorchMetrics is a really nice and convenient library that lets us compute the performance of models in an iterative fashion. It‚Äôs designed with PyTorch (and PyTorch Lightning) in mind, but it is a general-purpose library compatible with other libraries and workflows.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sebastianraschka.com/blog/2022/torchmetrics.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two minutes NLP ‚Äî Quick Introduction to Haystack</title>
      <link>/post/2022-04-09-two-minutes-nlp-quick-introduction-to-haystack/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-04-09-two-minutes-nlp-quick-introduction-to-haystack/</guid>
      <description>&lt;p&gt;Question Answering, Semantic Search, and the Retriever-Reader pipeline&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/nlplanet/two-minutes-nlp-quick-introduction-to-haystack-da86d0402998&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack/issues/486&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deepchecks Suite </title>
      <link>/post/2022-03-21-deepchecks-suite/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-deepchecks-suite/</guid>
      <description>&lt;p&gt;Deepchecks is the leading tool for validating your machine learning models and data, and it enables doing so with minimal effort. Deepchecks accompanies you through various validation needs such as verifying your data‚Äôs integrity, inspecting its distributions, validating data splits, evaluating your model and comparing between different models.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.deepchecks.com/en/stable/examples/guides/quickstart_in_5_minutes.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message Passing</title>
      <link>/post/2022-03-21-graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing/</guid>
      <description>&lt;p&gt;Physics-inspired continuous learning models on graphs allow to overcome the limitations of traditional GNNs&lt;/p&gt;
&lt;p&gt;The message-passing paradigm has been the ‚Äúbattle horse‚Äù of deep learning on graphs for several years, making graph neural networks a big success in a‚Ä¶&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-neural-networks-beyond-weisfeiler-lehman-and-vanilla-message-passing-bc8605fa59a&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph-based Fraud Detection Papers and Resources</title>
      <link>/post/2022-03-21-graph-based-fraud-detection-papers-and-resources/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-graph-based-fraud-detection-papers-and-resources/</guid>
      <description>&lt;p&gt;A curated list of fraud detection papers using graph information or graph neural networks&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/safe-graph/graph-fraud-detection-papers&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch VAE</title>
      <link>/post/2022-03-21-pytorch-vae/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-pytorch-vae/</guid>
      <description>&lt;p&gt;A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility.&lt;/p&gt;
&lt;p&gt;The aim of this project is to provide a quick and simple working example for many of the cool VAE models out there. All the models are trained on the CelebA dataset for consistency and comparison.&lt;/p&gt;
&lt;p&gt;The architecture of all the models are kept as similar as possible with the same layers, except for cases where the original paper necessitates a radically different architecture (Ex. VQ VAE uses Residual layers and no Batch-Norm, unlike other models). Here are the results of each model.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AntixK/PyTorch-VAE&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simplifying Transformer Research with xFormers &amp; Lightning</title>
      <link>/post/2022-03-21-simplifying-transformer-research-with-xformers-lightning/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-simplifying-transformer-research-with-xformers-lightning/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://devblog.pytorchlightning.ai/part-i-simplifying-transformer-research-with-xformers-lightning-a715737b8ad4&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VAE Playground</title>
      <link>/post/2022-03-21-vae-playground/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-vae-playground/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/reoneo97/vae-playground&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Visual Guide to Transformer Neural Networks</title>
      <link>/post/2022-03-21-visual-guide-to-transformer-neural-networks/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-visual-guide-to-transformer-neural-networks/</guid>
      <description>&lt;p&gt;Visual Guide to Transformer Neural Networks (Series) - Step by Step Intuitive Explanation&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gJ9kaJsE78k&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yann LeCun: &#34;A Path Towards Autonomous AI&#34;</title>
      <link>/post/2022-03-21-yann-lecun-a-path-towards-autonomous-ai/</link>
      <pubDate>Mon, 21 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-03-21-yann-lecun-a-path-towards-autonomous-ai/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=DokLw1tILlw&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Flask Web App for Automatic Text Summarization Using SBERT</title>
      <link>/post/2022-02-27-a-flask-web-app-for-automatic-text-summarization-using-sbert/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-27-a-flask-web-app-for-automatic-text-summarization-using-sbert/</guid>
      <description>&lt;p&gt;In this blog, we will build a Flask web app that can input any long piece of information such as a blog or news article and summarize it into just five lines!&lt;/p&gt;
&lt;p&gt;Text summarization is an NLP(Natural Language Processing) task. SBERT(Sentence-BERT) has been used to achieve the same.&lt;/p&gt;
&lt;p&gt;By the end of the article, you will learn how to integrate AI models and specifically pre-trained BERT models with Flask web technology as well! I will be explaining the step-by-step implementation right from the setup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2022/02/a-flask-web-app-for-automatic-text-summarization-using-sbert/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What does 2022 hold for Geometric &amp; Graph ML?</title>
      <link>/post/2022-02-27-what-does-2022-hold-for-geometric-graph-ml/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-27-what-does-2022-hold-for-geometric-graph-ml/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/predictions-and-hopes-for-geometric-graph-ml-in-2022-aa3b8b79f5cc&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Real-time face swap fpr PC streamig or video calls</title>
      <link>/post/2022-02-19-real-time-face-swap-fpr-pc-streamig-or-video-calls/</link>
      <pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-19-real-time-face-swap-fpr-pc-streamig-or-video-calls/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/iperov/DeepFaceLive&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The incredible pytorch</title>
      <link>/post/2022-02-19-the-incredible-pytorch/</link>
      <pubDate>Sat, 19 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/2022-02-19-the-incredible-pytorch/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.ritchieng.com/the-incredible-pytorch/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> AlphaFold2 Talk</title>
      <link>/post/2021-12-30-alphafold2-talk/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-alphafold2-talk/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/BurkovBA/AlphaFold2-talk&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A powerful and flexible machine learning platform for drug discovery</title>
      <link>/post/2021-12-30-a-powerful-and-flexible-machine-learning-platform-for-drug-discovery/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-a-powerful-and-flexible-machine-learning-platform-for-drug-discovery/</guid>
      <description>&lt;p&gt;TorchDrug is a machine learning platform designed for drug discovery, covering techniques from graph machine learning (graph neural networks, geometric deep learning &amp;amp; knowledge graphs), deep generative models to reinforcement learning. It provides a comprehensive and flexible interface to support rapid prototyping of drug discovery models in PyTorch.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://torchdrug.ai/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geometric Deep Learing</title>
      <link>/post/2021-12-30-geometric-deep-learing/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-geometric-deep-learing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://geometricdeeplearning.com/lectures/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph ML in 2022: Where Are We Now?</title>
      <link>/post/2021-12-30-graph-ml-in-2022-where-are-we-now/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-graph-ml-in-2022-where-are-we-now/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/graph-ml-in-2022-where-are-we-now-f7f8242599e0&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KG Course 2021</title>
      <link>/post/2021-12-30-kg-course-2021/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-kg-course-2021/</guid>
      <description>&lt;p&gt;–ö—É—Ä—Å –ø–æ –≥—Ä–∞—Ñ–∞–º –∑–Ω–∞–Ω–∏–π (Knowledge Graphs) –∏ –∫–∞–∫ –∏—Ö –≥–æ—Ç–æ–≤–∏—Ç—å –≤ 2021 –≥–æ–¥—É.
–ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://migalkin.github.io/kgcourse2021/#syllabus&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.</title>
      <link>/post/2021-12-30-pyg-pytorch-geometric-is-a-library-built-upon-pytorch-to-easily-write-and-train-graph-neural-networks-gnns-for-a-wide-range-of-applications-related-to-structured-data/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-pyg-pytorch-geometric-is-a-library-built-upon-pytorch-to-easily-write-and-train-graph-neural-networks-gnns-for-a-wide-range-of-applications-related-to-structured-data/</guid>
      <description>&lt;p&gt;It consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers. In addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support, distributed graph learning via Quiver, a large number of common benchmark datasets (based on simple interfaces to create your own), the GraphGym experiment manager, and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pytorch-geometric.readthedocs.io/en/latest/index.html&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyKEEN is a Python package for reproducible, facile knowledge graph embeddings.</title>
      <link>/post/2021-12-30-pykeen-is-a-python-package-for-reproducible-facile-knowledge-graph-embeddings/</link>
      <pubDate>Thu, 30 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-12-30-pykeen-is-a-python-package-for-reproducible-facile-knowledge-graph-embeddings/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pykeen.readthedocs.io/en/stable/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Complete Intuitive Guide To Transfer Learning </title>
      <link>/post/2021-11-14-a-complete-intuitive-guide-to-transfer-learning/</link>
      <pubDate>Sun, 14 Nov 2021 10:42:38 +0200</pubDate>
      
      <guid>/post/2021-11-14-a-complete-intuitive-guide-to-transfer-learning/</guid>
      <description>&lt;p&gt;Advancements in deep learning have been rapid over the past decade.&lt;/p&gt;
&lt;p&gt;While the discovery of neural networks happened almost six decades ago with the invention of the first artificial neural network in 1958 by psychologist Frank Rosenblatt (called the &amp;ldquo;perceptron&amp;rdquo;), the developments in the field did not gain true popularity until about a decade ago.&lt;/p&gt;
&lt;p&gt;The most popular achievement in 2009 was the creation of ImageNet. ImageNet is a humungous visual dataset that has led to some of the best modern-day deep learning and computer vision projects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/transfer-learning-explained/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Gentle Introduction to Anomaly Detection with Autoencoders</title>
      <link>/post/2021-11-14-a-gentle-introduction-to-anomaly-detection-with-autoencoders/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-a-gentle-introduction-to-anomaly-detection-with-autoencoders/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/autoencoder.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Anomagram is an interactive visualization tool for exploring how a deep learning model can be applied to the task of anomaly detection (on stationary data).&lt;/p&gt;
&lt;p&gt;Given an ECG signal sample, an autoencoder model (running live in your browser) can predict if it is normal or abnormal.&lt;/p&gt;
&lt;p&gt;To try it out, click any of the test ECG signals from the ECG5000 dataset below, or better still, draw a signal to see the model&amp;rsquo;s prediction!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://anomagram.fastforwardlabs.com/#/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Visual Guide to Using BERT for the First Time</title>
      <link>/post/2021-11-14-a-visual-guide-to-using-bert-for-the-first-time/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-a-visual-guide-to-using-bert-for-the-first-time/</guid>
      <description>&lt;p&gt;This post is a simple tutorial for how to use a variant of BERT to classify sentences.&lt;/p&gt;
&lt;p&gt;This is an example that is basic enough as a first intro, yet advanced enough to showcase some of the key concepts involved.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BMW TechOffice MUNICH</title>
      <link>/post/2021-11-14-bmw-techoffice-munich/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-bmw-techoffice-munich/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/repo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This organization contains software for realtime computer vision published by the members, partners and friends of the BMW TechOffice MUNICH and InnovationLab.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/BMW-InnovationLab&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Face Verification With Keras and Streamlit</title>
      <link>/post/2021-11-14-face-verification-with-keras-and-streamlit/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-face-verification-with-keras-and-streamlit/</guid>
      <description>&lt;p&gt;Streamlit enables data scientists and machine learning practitioners to build data and machine learning applications quickly.&lt;/p&gt;
&lt;p&gt;In this piece, we will look at how we can use Streamlit to build a face verification application.&lt;/p&gt;
&lt;p&gt;However, before we can start verifying faces, we have to detect them. In computer vision, face detection is the task of locating and localizing faces in an image. Face verification is the process of comparing the similarity of two or more images.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/face-verification-with-keras/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to fine-tune BERT to classify your Slack chats without coding</title>
      <link>/post/2021-11-14-how-to-fine-tune-bert-to-classify-your-slack-chats-without-coding/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-how-to-fine-tune-bert-to-classify-your-slack-chats-without-coding/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;images/lifecycle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Slack chats can become messy with time, proving difficult to extract meaningful information.&lt;/p&gt;
&lt;p&gt;In this article, I want to present a quick codeless way of fine-tuning and deploying the commonly used BERT classifier to do conversational analysis.&lt;/p&gt;
&lt;p&gt;We will use that system to extract tasks, facts, and other valuable information from our Slack conversations.&lt;/p&gt;
&lt;p&gt;It could be easily extended for categorizing any other textual data, like support requests, emails, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/how-to-finetune-bert-to-classify-your-slack-chats-without-coding-3a7002936bcf&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streamlit Demo: The Udacity Self-driving Car Image Browser</title>
      <link>/post/2021-11-14-streamlit-demo-the-udacity-self-driving-car-image-browser/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-streamlit-demo-the-udacity-self-driving-car-image-browser/</guid>
      <description>&lt;p&gt;This project demonstrates the Udacity self-driving-car dataset and YOLO object detection into an interactive Streamlit app.&lt;/p&gt;
&lt;p&gt;The complete demo is implemented in less than 300 lines of Python and illustrates all the major building blocks of Streamlit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/streamlit/demo-self-driving&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vision Transformers Explained</title>
      <link>/post/2021-11-14-vision-transformers-explained/</link>
      <pubDate>Sun, 14 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/post/2021-11-14-vision-transformers-explained/</guid>
      <description>&lt;p&gt;Introduced in the paper, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, Vision Transformers (ViT) are the new talk of the town for SOTA image classification.&lt;/p&gt;
&lt;p&gt;Experts feel this is only the tip of the iceberg when it comes to Transformer architectures replacing their convolutional counterparts for upstream/downstream tasks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.paperspace.com/vision-transformers/&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss> 
